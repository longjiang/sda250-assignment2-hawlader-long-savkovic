{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb56157",
   "metadata": {},
   "source": [
    "| Name (Last, First) | Student ID | Section contributed  | Section edited      | Other contributions   |\n",
    "|--------------------|------------|----------------------|---------------------|-----------------------|\n",
    "| Hawlader, Antanila | 301332035  | Researched the codes | finding datasets    | provided functions    |\n",
    "| Long, Jiang        | 200099436  | Inputted the codes   | edited all sections | inputted functions    |\n",
    "| Savkovic, Sava     | 301397121  | Research codes       | researched reviews  | reviewed all sections |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3d6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd2089",
   "metadata": {},
   "source": [
    "The solution to the assignment is mostly based on the Datacamp tutorial [Python sentiment analysis tutorial](https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python) by Sayak Paul (Sayak, 2021). The tutorial is a \"bag-of-words\" approach with supervised machine learning using Naive Bayes. We have adapted the tutorial to perform sentimental analysis on a large set of Rotten Tomatoes reviews downloaded from [Rotten Tomatoes movies and critic reviews dataset](https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset) from Kaggle as a large CSV (Leone, 2020).\n",
    "\n",
    "We have truncated the CSV file so it's less than 100 MB so it will be under GitHub's file size limit. The truncated CSV contains 485,212 movie reviews from Rotten Tomatoes, each labeled 'Fresh' (positive) or 'Rotten' (negative). But we will only work on the first 128,000 movie reviews so the process won't take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8be591",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_documents = 128000 # 'None' means no limit. Tokenizing all 485,212 reviews will take a LONG TIME\n",
    "\n",
    "file_path = 'rotten-tomatoes-movies-and-critic-reviews-dataset/rotten_tomatoes_critic_reviews.csv'\n",
    "\n",
    "# Open the file again for reading\n",
    "f = open(file_path, encoding='UTF8')\n",
    "csv_reader = csv.DictReader(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af94725",
   "metadata": {},
   "source": [
    "We read the CSV file line by line. For each line, we store the record as a set. The first element is tokenized review text, the second element is either 'pos' for positive ('fresh') reviews or 'neg' for negative ('rotten') reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents_text = ''\n",
    "\n",
    "for line in itertools.islice(csv_reader, 0, max_documents):\n",
    "    # For some reason '10,000' shows up as an \"informative feature\", let's ignore comments with it\n",
    "    if \"10,000\" in line['review_content']: \n",
    "        continue\n",
    "    if(line['review_type']):\n",
    "        sentiment = False\n",
    "        if line['review_type'] == 'Fresh':\n",
    "            sentiment = 'pos'\n",
    "        if line['review_type'] == 'Rotten':\n",
    "            sentiment = 'neg'\n",
    "        if(sentiment):\n",
    "            tokens = nltk.word_tokenize(line['review_content'])\n",
    "            documents_text = documents_text + line['review_content']\n",
    "            documents.append((tokens, sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9c0bf",
   "metadata": {},
   "source": [
    "In the mean time, we concatenate all the reviews we looked at in a a single long string `documents_text`, and tokenize it into `documents_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_tokens = nltk.word_tokenize(documents_text)\n",
    "\n",
    "print(f\"Using {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697dce1",
   "metadata": {},
   "source": [
    "Next, we will retrieve the most frequently occurring 2,000 words in the reviews we looked at. Theses words will be used for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_features = 2000\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in documents_tokens)\n",
    "\n",
    "word_features = list(all_words)[:max_word_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc7cd8",
   "metadata": {},
   "source": [
    "We then use the 2,000 words to extract features. We do this by going through each review (`document`). For each review, we add a feature `contains(WORD)` for each of the 2,000 `WORD`s: `contains(the)`, `contains(a)`, `contains(it)`, etc. Therefore, each review will have 2,000 features. We store these in a list of sets, the first element in the set being the 2,000 features, and the second element in the set being the class (`pos` or `neg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66472d41",
   "metadata": {},
   "source": [
    "We will split our corpus up into a training set and a test set. The training set contains 90% of the reviews, and the testing set 10% of the reviews.\n",
    "\n",
    "We will train a Naive Bayes `classifier` on the training set, and try it out on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fdf361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ratio = 0.9 # 0.9 means that 90% of the data from the corpus is used for training, 10% is for testing\n",
    "\n",
    "train_set_size = math.floor(len(documents) * train_set_ratio)\n",
    "\n",
    "train_set, test_set = featuresets[:train_set_size], featuresets[train_set_size:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ece920",
   "metadata": {},
   "source": [
    "We now test the classifier. We tried changing different parameters including `max_documents` and `max_word_features`. We find that as we feed more training data to model, accuracy score will approach around 70%. In this case we used `max_documents = 128000` and `max_word_features = 2000` and we got an accuracy score of 72%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4b1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7271803688652704\n",
      "Most Informative Features\n",
      "       contains(unfunny) = True              neg : pos    =     67.2 : 1.0\n",
      "      contains(superbly) = True              pos : neg    =     28.3 : 1.0\n",
      "        contains(deftly) = True              pos : neg    =     24.9 : 1.0\n",
      "  contains(refreshingly) = True              pos : neg    =     21.1 : 1.0\n",
      "           contains(gem) = True              pos : neg    =     16.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b41e22",
   "metadata": {},
   "source": [
    "Finally, we show the most significant features that affect the classifier outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c946ac3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Paul, Sayak. (2021, May 17) Python sentiment analysis tutorial. DataCamp Community. (n.d.). Retrieved March 10, 2022, from https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python\n",
    "\n",
    "Leone, S. (2020, November 4). Rotten tomatoes movies and critic Reviews Dataset. Kaggle. Retrieved March 10, 2022, from https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d1ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
