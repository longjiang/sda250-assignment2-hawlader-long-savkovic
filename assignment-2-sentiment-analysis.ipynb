{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb56157",
   "metadata": {},
   "source": [
    "| Name (Last, First) | Student ID | Section contributed  | Section edited      | Other contributions   |\n",
    "|--------------------|------------|----------------------|---------------------|-----------------------|\n",
    "| Hawlader, Antanila | 301332035  | Researched the codes | finding datasets    | provided functions    |\n",
    "| Long, Jiang        | 200099436  | Inputted the codes   | edited all sections | inputted functions    |\n",
    "| Savkovic, Sava     | 301397121  | Research codes       | researched reviews  | reviewed all sections |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fcfe8",
   "metadata": {},
   "source": [
    "The solution to the assignment is mostly based on the Datacamp tutorial [Python sentiment analysis tutorial](https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python) by Sayak Paul (Sayak, 2021). The tutorial is a \"bag-of-words\" approach with supervised machine learning using Naive Bayes. We have adapted the tutorial to perform sentimental analysis on a large set of Rotten Tomatoes reviews downloaded from [Rotten Tomatoes movies and critic reviews dataset](https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset) from Kaggle as a large CSV.\n",
    "\n",
    "We have truncated the CSV file so it's less than 100 MB so it will be under GitHub's file size limit. The truncated CSV contains 485,212 movie reviews from Rotten Tomatoes, each labeled 'Fresh' (positive) or 'Rotten' (negative). But we will only work on the first 128,000 movie reviews so the process won't take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_documents = 128000 # 'None' means no limit. Tokenizing all 485,212 reviews will take a LONG TIME\n",
    "\n",
    "file_path = 'rotten-tomatoes-movies-and-critic-reviews-dataset/rotten_tomatoes_critic_reviews.csv'\n",
    "\n",
    "# Open the file again for reading\n",
    "f = open(file_path, encoding='UTF8')\n",
    "csv_reader = csv.DictReader(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f72c7d",
   "metadata": {},
   "source": [
    "We read the CSV file line by line. For each line, we store the record as a set. The first element is tokenized review text, the second element is either 'pos' for positive ('fresh') reviews or 'neg' for negative ('rotten') reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fc33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 127959 documents.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "documents_text = ''\n",
    "\n",
    "for line in itertools.islice(csv_reader, 0, max_documents):\n",
    "    # For some reason '10,000' shows up as an \"informative feature\", let's ignore comments with it\n",
    "    if \"10,000\" in line['review_content']: \n",
    "        continue\n",
    "    if(line['review_type']):\n",
    "        sentiment = False\n",
    "        if line['review_type'] == 'Fresh':\n",
    "            sentiment = 'pos'\n",
    "        if line['review_type'] == 'Rotten':\n",
    "            sentiment = 'neg'\n",
    "        if(sentiment):\n",
    "            tokens = nltk.word_tokenize(line['review_content'])\n",
    "            documents_text = documents_text + line['review_content']\n",
    "            documents.append((tokens, sentiment))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e8f3b",
   "metadata": {},
   "source": [
    "We also concatenate all the reviews we looked at in a a single long string `documents_text`, and tokenize it into `documents_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_tokens = nltk.word_tokenize(documents_text)\n",
    "print(f\"Using {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0a24f",
   "metadata": {},
   "source": [
    "Then, the funtions of features, training set and test sets will determine whether there are more positive or negative words in the dataset.\n",
    "\n",
    "We will also look at the accuracy representation of the word features .\n",
    "\n",
    "Next, after determining the positive and negative reviews from the corpus we will begin to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fccb63",
   "metadata": {},
   "source": [
    "This step is known as document classification.\n",
    "\n",
    "We will retrieve the top 2000 words. This includes both positive and negative data.\n",
    "\n",
    "This means we will need to make a frequency distribution of the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_features = 2000\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in documents_tokens)\n",
    "word_features = list(all_words)[:max_word_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54a2b8",
   "metadata": {},
   "source": [
    "We will try the tags document_features and document_words to distinguish the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c20926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab36f8",
   "metadata": {},
   "source": [
    "We will split our corpus up into a training set and a test set. \n",
    "\n",
    "To set up a tagger something called training. We will train the tagger on a training set and evaluate it on a test set.\n",
    "\n",
    "The training set is used to train the initial features\n",
    "\n",
    "Test test is used for final testing and no features are deprived from this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fdf361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ratio = 0.9 # 0.9 means that 90% of the data from the corpus is used for training, 10% is for testing\n",
    "\n",
    "train_set_size = math.floor(len(documents) * train_set_ratio)\n",
    "\n",
    "train_set, test_set = featuresets[:train_set_size], featuresets[train_set_size:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2f024",
   "metadata": {},
   "source": [
    "We then test the classifier, and show the most important features as interpreted by Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4b1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7271803688652704\n",
      "Most Informative Features\n",
      "       contains(unfunny) = True              neg : pos    =     67.2 : 1.0\n",
      "      contains(superbly) = True              pos : neg    =     28.3 : 1.0\n",
      "        contains(deftly) = True              pos : neg    =     24.9 : 1.0\n",
      "  contains(refreshingly) = True              pos : neg    =     21.1 : 1.0\n",
      "           contains(gem) = True              pos : neg    =     16.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    " \n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c946ac3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Paul, Sayak. (2021, May 17) Python sentiment analysis tutorial. DataCamp Community. (n.d.). Retrieved March 10, 2022, from https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python\n",
    "\n",
    "Leone, S. (2020, November 4). Rotten tomatoes movies and critic Reviews Dataset. Kaggle. Retrieved March 10, 2022, from https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
